"""
æµ‹è¯•TSE_losså‡½æ•°çš„æ­£ç¡®æ€§
éªŒè¯:
1. è¾“å…¥è¾“å‡ºç»´åº¦
2. æ©ç ç”Ÿæˆé€»è¾‘
3. é˜ˆå€¼è®¡ç®—
4. æŸå¤±å€¼åˆç†æ€§
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))
from functions import TSE_loss


def test_tse_loss_basic():
    """åŸºæœ¬åŠŸèƒ½æµ‹è¯•"""
    print("=" * 60)
    print("æµ‹è¯•1: TSE_lossåŸºæœ¬åŠŸèƒ½")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    
    # æ¨¡æ‹Ÿå‚æ•°
    B, T, C, H, W = 4, 4, 128, 8, 8  # Batch, Time, Channel, Height, Width
    num_classes = 10
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    feature_maps = torch.randn(B, T, C, H, W)
    labels = torch.randint(0, num_classes, (B,))
    
    # åˆ›å»ºåˆ†ç±»å±‚
    fc_layer = nn.Linear(C, num_classes)
    criterion = nn.CrossEntropyLoss()
    
    print(f"è¾“å…¥ç‰¹å¾å›¾å½¢çŠ¶: {feature_maps.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    print(f"åˆ†ç±»å±‚: Linear({C}, {num_classes})")
    
    # è®¡ç®—TSEæŸå¤±
    try:
        loss = TSE_loss(
            feature_maps=feature_maps,
            fc_layers=fc_layer,
            labels=labels,
            criterion=criterion,
            tau_f=0.5,
            kappa=1.0
        )
        print(f"\nâœ… TSEæŸå¤±è®¡ç®—æˆåŠŸ!")
        print(f"æŸå¤±å€¼: {loss.item():.4f}")
        print(f"æŸå¤±ç±»å‹: {type(loss)}")
        print(f"æŸå¤±requires_grad: {loss.requires_grad}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        loss.backward()
        print("\nâœ… åå‘ä¼ æ’­æˆåŠŸ!")
        
        return True
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_tse_threshold_logic():
    """æµ‹è¯•é˜ˆå€¼è®¡ç®—é€»è¾‘"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: é˜ˆå€¼è®¡ç®—é€»è¾‘")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    B, T, C, H, W = 2, 3, 64, 4, 4
    num_classes = 5
    
    feature_maps = torch.randn(B, T, C, H, W)
    labels = torch.tensor([1, 3])  # å›ºå®šæ ‡ç­¾ä¾¿äºåˆ†æ
    
    # å•å±‚FC (ç”¨äºç®€å•æµ‹è¯•)
    fc_layers = nn.Linear(C, num_classes)
    criterion = nn.CrossEntropyLoss()
    
    # ä¸åŒçš„tau_få’Œkappa
    test_params = [
        (0.3, 0.5, "ä½å›ºå®šé˜ˆå€¼,ä½åŠ¨æ€ç³»æ•°"),
        (0.5, 1.0, "ä¸­ç­‰(é»˜è®¤)"),
        (0.7, 2.0, "é«˜å›ºå®šé˜ˆå€¼,é«˜åŠ¨æ€ç³»æ•°"),
    ]
    
    print("\nä¸åŒè¶…å‚æ•°ä¸‹çš„æŸå¤±å€¼:")
    print(f"{'tau_f':<8} {'kappa':<8} {'Loss':<12} {'æè¿°':<20}")
    print("-" * 60)
    
    for tau_f, kappa, desc in test_params:
        loss = TSE_loss(
            feature_maps=feature_maps,
            fc_layers=fc_layers,
            labels=labels,
            criterion=criterion,
            tau_f=tau_f,
            kappa=kappa
        )
        print(f"{tau_f:<8.1f} {kappa:<8.1f} {loss.item():<12.4f} {desc:<20}")
    
    print("\nâœ… é˜ˆå€¼é€»è¾‘æµ‹è¯•å®Œæˆ")
    return True


def test_tse_time_independence():
    """æµ‹è¯•ä¸åŒæ—¶é—´æ­¥çš„ç‹¬ç«‹ç›‘ç£"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: æ—¶é—´æ­¥ç‹¬ç«‹ç›‘ç£")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    B, C, H, W = 2, 64, 4, 4
    num_classes = 5
    labels = torch.tensor([1, 3])
    
    fc_layer = nn.Linear(C, num_classes)
    criterion = nn.CrossEntropyLoss()
    
    # æµ‹è¯•ä¸åŒçš„æ—¶é—´æ­¥æ•°
    time_steps = [2, 4, 8]
    
    print("\nä¸åŒæ—¶é—´æ­¥æ•°çš„æŸå¤±å€¼:")
    print(f"{'Time Steps':<15} {'Loss':<12}")
    print("-" * 30)
    
    for T in time_steps:
        feature_maps = torch.randn(B, T, C, H, W)
        loss = TSE_loss(
            feature_maps=feature_maps,
            fc_layers=fc_layer,
            labels=labels,
            criterion=criterion,
            tau_f=0.5,
            kappa=1.0
        )
        print(f"{T:<15} {loss.item():<12.4f}")
    
    print("\nâœ… æ—¶é—´æ­¥ç‹¬ç«‹æ€§æµ‹è¯•å®Œæˆ")
    return True


def test_tse_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æ¢¯åº¦æµ")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    B, T, C, H, W = 2, 4, 64, 4, 4
    num_classes = 5
    
    feature_maps = torch.randn(B, T, C, H, W, requires_grad=True)
    labels = torch.tensor([1, 3])
    
    fc_layer = nn.Linear(C, num_classes)
    criterion = nn.CrossEntropyLoss()
    
    # è®¡ç®—æŸå¤±
    loss = TSE_loss(
        feature_maps=feature_maps,
        fc_layers=fc_layer,
        labels=labels,
        criterion=criterion,
        tau_f=0.5,
        kappa=1.0
    )
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print(f"Feature mapsæ¢¯åº¦å½¢çŠ¶: {feature_maps.grad.shape}")
    print(f"æ¢¯åº¦æ˜¯å¦å…¨ä¸º0: {torch.all(feature_maps.grad == 0).item()}")
    print(f"æ¢¯åº¦æœ€å¤§å€¼: {feature_maps.grad.abs().max().item():.6f}")
    print(f"æ¢¯åº¦æœ€å°å€¼: {feature_maps.grad.abs().min().item():.6f}")
    print(f"æ¢¯åº¦å¹³å‡å€¼: {feature_maps.grad.abs().mean().item():.6f}")
    
    # æ£€æŸ¥fcå±‚æ¢¯åº¦
    print(f"\nFCå±‚æƒé‡æ¢¯åº¦å½¢çŠ¶: {fc_layer.weight.grad.shape}")
    print(f"FCå±‚æ¢¯åº¦æ˜¯å¦å…¨ä¸º0: {torch.all(fc_layer.weight.grad == 0).item()}")
    print(f"FCå±‚æ¢¯åº¦å¹³å‡å€¼: {fc_layer.weight.grad.abs().mean().item():.6f}")
    
    assert not torch.all(feature_maps.grad == 0), "Feature mapsæ¢¯åº¦ä¸åº”å…¨ä¸º0"
    assert not torch.all(fc_layer.weight.grad == 0), "FCå±‚æ¢¯åº¦ä¸åº”å…¨ä¸º0"
    
    print("\nâœ… æ¢¯åº¦æµæµ‹è¯•é€šè¿‡")
    return True


def test_tse_mask_generation():
    """æµ‹è¯•æ©ç ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5: æ©ç ç”Ÿæˆå’Œå¯è§†åŒ–")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    # ç®€åŒ–å‚æ•°ä¾¿äºåˆ†æ
    B, T, C, H, W = 1, 3, 32, 4, 4
    num_classes = 3
    labels = torch.tensor([1])  # çœŸå®ç±»åˆ«ä¸º1
    
    # åˆ›å»ºç‰¹å¾å›¾,ä½¿å…¶æœ‰æ˜æ˜¾çš„ç©ºé—´æ¨¡å¼
    feature_maps = torch.randn(B, T, C, H, W)
    # è®©æŸäº›ä½ç½®æœ‰æ›´å¼ºçš„æ¿€æ´»
    feature_maps[0, 1, :, 0, 0] *= 2.0  # å·¦ä¸Šè§’å¢å¼º
    feature_maps[0, 2, :, 3, 3] *= 2.0  # å³ä¸‹è§’å¢å¼º
    
    fc_layer = nn.Linear(C, num_classes)
    criterion = nn.CrossEntropyLoss()
    
    print("æ¨¡æ‹Ÿæ©ç ç”Ÿæˆè¿‡ç¨‹:")
    print("-" * 60)
    
    # æ‰‹åŠ¨æ¨¡æ‹ŸTSEçš„æ©ç ç”Ÿæˆ
    with torch.no_grad():
        # å¯¹ç¬¬äºŒä¸ªæ—¶é—´æ­¥(t=1)
        t = 1
        B, C, H, W = feature_maps.shape[0], feature_maps.shape[2], feature_maps.shape[3], feature_maps.shape[4]
        
        # è®¡ç®—åˆ†ç±»é¢„æµ‹å›¾
        features_t = feature_maps[:, t]  # [B, C, H, W]
        B_size, C_size, H_size, W_size = features_t.shape
        
        # é‡å¡‘ä¸º [B*H*W, C]
        features_flat = features_t.permute(0, 2, 3, 1).reshape(B_size * H_size * W_size, C_size)
        
        # åˆ†ç±»
        predictions = fc_layer(features_flat)  # [B*H*W, num_classes]
        predictions = predictions.reshape(B_size, H_size, W_size, num_classes)  # [B, H, W, num_classes]
        predictions = predictions.permute(0, 3, 1, 2)  # [B, num_classes, H, W]
        
        # å¹³å‡ä¹‹å‰çš„é¢„æµ‹(è¿™é‡Œåªæœ‰t=0)
        prev_features = feature_maps[:, 0]
        prev_flat = prev_features.permute(0, 2, 3, 1).reshape(B_size * H_size * W_size, C_size)
        prev_pred = fc_layer(prev_flat).reshape(B_size, H_size, W_size, num_classes).permute(0, 3, 1, 2)
        
        avg_pred = prev_pred
        avg_prob = torch.softmax(avg_pred, dim=1)  # [B, num_classes, H, W]
        
        # æå–çœŸå®ç±»åˆ«çš„æ¦‚ç‡å›¾
        prob_map = avg_prob[torch.arange(B_size), labels]  # [B, H, W]
        
        print(f"æ—¶é—´æ­¥ t={t} çš„æ¦‚ç‡å›¾ (çœŸå®ç±»åˆ«={labels.item()}):")
        print(f"å½¢çŠ¶: {prob_map.shape}")
        print(f"æ¦‚ç‡å›¾ (4x4):\n{prob_map[0].numpy()}")
        
        # è®¡ç®—é˜ˆå€¼
        tau_f = 0.5
        kappa = 1.0
        mean_prob = prob_map.mean()
        std_prob = prob_map.std()
        tau_d = mean_prob + kappa * std_prob
        threshold = max(tau_f, tau_d.item())
        
        print(f"\né˜ˆå€¼è®¡ç®—:")
        print(f"  å›ºå®šé˜ˆå€¼ Ï„_f: {tau_f:.4f}")
        print(f"  å‡å€¼: {mean_prob.item():.4f}")
        print(f"  æ ‡å‡†å·®: {std_prob.item():.4f}")
        print(f"  åŠ¨æ€é˜ˆå€¼ Ï„_d: {tau_d.item():.4f}")
        print(f"  æœ€ç»ˆé˜ˆå€¼: {threshold:.4f}")
        
        # ç”Ÿæˆæ©ç 
        mask = (prob_map < threshold).float()
        print(f"\næ©ç  (1=ä¿ç•™, 0=æ“¦é™¤):")
        print(mask[0].numpy())
        
        erased_ratio = (mask == 0).sum().item() / mask.numel()
        print(f"\næ“¦é™¤æ¯”ä¾‹: {erased_ratio * 100:.1f}%")
    
    print("\nâœ… æ©ç ç”Ÿæˆæµ‹è¯•å®Œæˆ")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("TSE_loss å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    tests = [
        ("åŸºæœ¬åŠŸèƒ½", test_tse_loss_basic),
        ("é˜ˆå€¼é€»è¾‘", test_tse_threshold_logic),
        ("æ—¶é—´æ­¥ç‹¬ç«‹æ€§", test_tse_time_independence),
        ("æ¢¯åº¦æµ", test_tse_gradient_flow),
        ("æ©ç ç”Ÿæˆ", test_tse_mask_generation),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
    
    total = len(results)
    passed = sum(1 for _, s in results if s)
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! TSE_losså®ç°æ­£ç¡®ã€‚")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥,è¯·æ£€æŸ¥å®ç°ã€‚")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
